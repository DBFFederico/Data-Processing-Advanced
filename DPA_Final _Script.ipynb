{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Skills: Data Processing Advanced\n",
    "## Group 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Solution:\n",
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import nltk\n",
    "import sklearn.neighbors\n",
    "import scipy.spatial.distance\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"competition_descriptions.txt\") as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Punctuation & Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(sentences):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize \n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk.stem import WordNetLemmatizer     \n",
    "\n",
    "    stop_words = set(stopwords.words('english')) - {'he', 'she', 'her', 'his', 'they', 'not'}\n",
    "    filtered = []\n",
    "\n",
    "    for each in range(len(sentences)):\n",
    "        sentences[each] = sentences[each].replace(\"[comma]\", \"\").strip()\n",
    "        filtered_sentence = []\n",
    "        comment_text = sentences[each]\n",
    "        # Remove punctuation & non-alphabetic characters\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        # Remove stop-words\n",
    "        word_tokens = tokenizer.tokenize(comment_text.lower())\n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                w = lemmatizer.lemmatize(w)\n",
    "                filtered_sentence.append(w)\n",
    "        filtered.append(filtered_sentence)\n",
    "    return filtered\n",
    "\n",
    "words = getWords(content)\n",
    "tokens = []\n",
    "for each in range(len(words)):\n",
    "    tokens.append(nltk.pos_tag(words[each]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parts of speech tags\n",
    "words = getWords(content)\n",
    "tokens = []\n",
    "for each in range(len(words)):\n",
    "    tokens.append(nltk.pos_tag(words[each]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat PoS tags to use with the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def getWordnetPos(tags):\n",
    "    for each in range(len(tags)):\n",
    "        #print(tags[each])\n",
    "        for i in range(len(tags[each])):\n",
    "            if tags[each][i][1].startswith('J'):\n",
    "                tags[each][i] += (str(wordnet.ADJ), )\n",
    "            #print(tags[each])\n",
    "            elif tags[each][i][1].startswith('V'):\n",
    "                tags[each][i] += (str(wordnet.VERB), )\n",
    "            #print(tags[each])\n",
    "            elif tags[each][i][1].startswith('N'):\n",
    "                tags[each][i] += (str(wordnet.NOUN), )\n",
    "            #print(tags[each])\n",
    "            elif tags[each][i][1].startswith('R'):\n",
    "                tags[each][i] += (str(wordnet.ADV), )\n",
    "            #print(tags[each])\n",
    "            else:\n",
    "                tags[each][i] += (str(\"\"), )\n",
    "            #print(tags[each])\n",
    "            \n",
    "    return tags\n",
    "\n",
    "tokenized_words = getWordnetPos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking -> look | position: v\n",
      "made -> make | position: v\n",
      "combed -> comb | position: v\n",
      "kept -> keep | position: v\n",
      "wearing -> wear | position: v\n",
      "pulled -> pull | position: v\n",
      "looking -> look | position: v\n",
      "looking -> look | position: v\n",
      "groomed -> groom | position: v\n",
      "left -> leave | position: v\n",
      "bit -> bite | position: v\n",
      "pulled -> pull | position: v\n",
      "colored -> color | position: v\n",
      "wearing -> wear | position: v\n",
      "shaped -> shape | position: v\n",
      "sized -> size | position: v\n",
      "seems -> seem | position: v\n",
      "left -> leave | position: v\n",
      "looking -> look | position: v\n",
      "organized -> organize | position: v\n",
      "curlier -> curly | position: a\n",
      "observes -> observe | position: v\n",
      "looking -> look | position: v\n",
      "dating -> date | position: v\n",
      "peaked -> peak | position: v\n",
      "sized -> size | position: v\n",
      "leaning -> lean | position: v\n",
      "appears -> appear | position: v\n",
      "colored -> color | position: v\n",
      "requires -> require | position: v\n",
      "piercing -> pierce | position: v\n",
      "tied -> tie | position: v\n",
      "combed -> comb | position: v\n",
      "piercing -> pierce | position: v\n",
      "grew -> grow | position: v\n",
      "cleaned -> clean | position: v\n",
      "trimmed -> trim | position: v\n",
      "seems -> seem | position: v\n",
      "seems -> seem | position: v\n",
      "trimmed -> trim | position: v\n",
      "pieced -> piece | position: v\n",
      "identifying -> identify | position: v\n",
      "appears -> appear | position: v\n",
      "aged -> age | position: v\n",
      "rounded -> round | position: v\n",
      "larger -> large | position: a\n",
      "swept -> sweep | position: v\n",
      "curved -> curve | position: v\n",
      "spends -> spend | position: v\n",
      "looking -> look | position: v\n",
      "skinnier -> skinny | position: a\n",
      "hunched -> hunch | position: v\n",
      "sticking -> stick | position: v\n",
      "longer -> long | position: a\n",
      "pointed -> point | position: v\n",
      "appears -> appear | position: v\n",
      "standing -> stand | position: v\n",
      "shaped -> shape | position: v\n",
      "maintains -> maintain | position: v\n",
      "maintained -> maintain | position: v\n",
      "touched -> touch | position: v\n",
      "seems -> seem | position: v\n",
      "looking -> look | position: v\n",
      "appears -> appear | position: v\n",
      "arched -> arch | position: v\n",
      "pierced -> pierce | position: v\n",
      "seems -> seem | position: v\n",
      "seems -> seem | position: v\n",
      "intimidating -> intimidate | position: v\n",
      "headed -> head | position: v\n",
      "smaller -> small | position: a\n",
      "framed -> frame | position: v\n",
      "groomed -> groom | position: v\n",
      "arched -> arch | position: v\n",
      "bigger -> big | position: a\n",
      "seems -> seem | position: v\n",
      "aged -> age | position: v\n",
      "depressed -> depress | position: v\n",
      "aged -> age | position: v\n",
      "colored -> color | position: v\n",
      "tied -> tie | position: v\n",
      "shaped -> shape | position: v\n",
      "arched -> arch | position: v\n",
      "cropped -> crop | position: v\n",
      "looking -> look | position: v\n",
      "looking -> look | position: v\n",
      "younger -> young | position: a\n",
      "appears -> appear | position: v\n",
      "wearing -> wear | position: v\n",
      "seems -> seem | position: v\n",
      "shaved -> shave | position: v\n",
      "pulled -> pull | position: v\n",
      "styled -> style | position: v\n",
      "boned -> bone | position: v\n",
      "wearing -> wear | position: v\n",
      "weighing -> weigh | position: v\n",
      "tanned -> tan | position: v\n",
      "arched -> arch | position: v\n",
      "curled -> curl | position: v\n",
      "scaring -> scar | position: v\n",
      "tired -> tire | position: v\n",
      "groomed -> groom | position: v\n",
      "seems -> seem | position: v\n",
      "parted -> part | position: v\n",
      "appears -> appear | position: v\n",
      "smiling -> smile | position: v\n",
      "pierced -> pierce | position: v\n",
      "making -> make | position: v\n",
      "needed -> need | position: v\n",
      "cropped -> crop | position: v\n",
      "longer -> long | position: a\n",
      "appears -> appear | position: v\n",
      "aged -> age | position: v\n",
      "looking -> look | position: v\n",
      "younger -> young | position: a\n",
      "pulled -> pull | position: v\n",
      "making -> make | position: v\n",
      "frowning -> frown | position: v\n",
      "wearing -> wear | position: v\n",
      "looking -> look | position: v\n",
      "shocked -> shock | position: v\n",
      "seems -> seem | position: v\n",
      "seems -> seem | position: v\n",
      "aged -> age | position: v\n",
      "headed -> head | position: v\n",
      "wearing -> wear | position: v\n",
      "saying -> say | position: v\n",
      "pointed -> point | position: v\n",
      "wearing -> wear | position: v\n",
      "rounded -> round | position: v\n",
      "pulled -> pull | position: v\n",
      "crooked -> crook | position: v\n",
      "looking -> look | position: v\n",
      "looking -> look | position: v\n",
      "getting -> get | position: v\n",
      "detected -> detect | position: v\n",
      "defined -> define | position: v\n",
      "chiseled -> chisel | position: v\n",
      "groomed -> groom | position: v\n",
      "sized -> size | position: v\n",
      "looking -> look | position: v\n",
      "growing -> grow | position: v\n",
      "looking -> look | position: v\n",
      "tied -> tie | position: v\n",
      "tired -> tire | position: v\n",
      "shorter -> short | position: a\n",
      "pulled -> pull | position: v\n",
      "got -> get | position: v\n",
      "crooked -> crook | position: v\n",
      "hidden -> hide | position: v\n",
      "flipped -> flip | position: v\n",
      "arched -> arch | position: v\n",
      "seems -> seem | position: v\n",
      "pulled -> pull | position: v\n",
      "shaped -> shape | position: v\n",
      "seems -> seem | position: v\n",
      "longer -> long | position: a\n",
      "pulled -> pull | position: v\n",
      "larger -> large | position: a\n",
      "toned -> tone | position: v\n",
      "built -> build | position: v\n",
      "leaning -> lean | position: v\n",
      "pursed -> purse | position: v\n",
      "parted -> part | position: v\n",
      "shaved -> shave | position: v\n",
      "wearing -> wear | position: v\n",
      "written -> write | position: v\n",
      "looking -> look | position: v\n",
      "sized -> size | position: v\n",
      "frowning -> frown | position: v\n",
      "tied -> tie | position: v\n",
      "shaped -> shape | position: v\n",
      "dressed -> dress | position: v\n",
      "pulled -> pull | position: v\n",
      "groomed -> groom | position: v\n",
      "looking -> look | position: v\n",
      "longer -> long | position: a\n",
      "pronounced -> pronounce | position: v\n",
      "shaped -> shape | position: v\n",
      "smaller -> small | position: a\n",
      "pulled -> pull | position: v\n",
      "seems -> seem | position: v\n",
      "going -> go | position: v\n",
      "appears -> appear | position: v\n",
      "balding -> bald | position: v\n",
      "tied -> tie | position: v\n",
      "scowling -> scowl | position: v\n",
      "appears -> appear | position: v\n",
      "groomed -> groom | position: v\n",
      "tied -> tie | position: v\n",
      "smiling -> smile | position: v\n",
      "flushed -> flush | position: v\n",
      "wrinkled -> wrinkle | position: v\n",
      "shaped -> shape | position: v\n",
      "wearing -> wear | position: v\n",
      "built -> build | position: v\n",
      "starting -> start | position: v\n",
      "skinned -> skin | position: v\n",
      "bit -> bite | position: v\n",
      "sized -> size | position: v\n",
      "wearing -> wear | position: v\n",
      "mentioned -> mention | position: v\n",
      "disabled -> disable | position: v\n",
      "wearing -> wear | position: v\n",
      "wearing -> wear | position: v\n",
      "written -> write | position: v\n",
      "looked -> look | position: v\n",
      "appears -> appear | position: v\n",
      "colored -> color | position: v\n",
      "looking -> look | position: v\n",
      "angled -> angle | position: v\n",
      "seems -> seem | position: v\n",
      "appears -> appear | position: v\n",
      "pointed -> point | position: v\n",
      "skinned -> skin | position: v\n",
      "dyed -> dye | position: v\n",
      "caring -> care | position: v\n",
      "appears -> appear | position: v\n",
      "cropped -> crop | position: v\n",
      "appears -> appear | position: v\n",
      "older -> old | position: a\n",
      "colored -> color | position: v\n",
      "wearing -> wear | position: v\n",
      "tired -> tire | position: v\n",
      "groomed -> groom | position: v\n",
      "groomed -> groom | position: v\n",
      "smaller -> small | position: a\n",
      "weighs -> weigh | position: v\n",
      "groomed -> groom | position: v\n",
      "wearing -> wear | position: v\n",
      "seems -> seem | position: v\n",
      "larger -> large | position: a\n",
      "wrinkling -> wrinkle | position: v\n",
      "skinned -> skin | position: v\n",
      "weighs -> weigh | position: v\n",
      "looking -> look | position: v\n",
      "cropped -> crop | position: v\n",
      "looking -> look | position: v\n",
      "looking -> look | position: v\n",
      "wearing -> wear | position: v\n",
      "seems -> seem | position: v\n",
      "boring -> bore | position: v\n",
      "defined -> define | position: v\n",
      "skinned -> skin | position: v\n",
      "bit -> bite | position: v\n",
      "pictured -> picture | position: v\n",
      "stylized -> stylize | position: v\n",
      "styled -> style | position: v\n",
      "shaved -> shave | position: v\n",
      "aged -> age | position: v\n",
      "looking -> look | position: v\n",
      "rimmed -> rim | position: v\n",
      "groomed -> groom | position: v\n",
      "smaller -> small | position: a\n",
      "shaved -> shave | position: v\n",
      "coming -> come | position: v\n",
      "shaved -> shave | position: v\n",
      "shorter -> short | position: a\n",
      "parted -> part | position: v\n",
      "hidden -> hide | position: v\n",
      "combed -> comb | position: v\n",
      "wearing -> wear | position: v\n",
      "trimmed -> trim | position: v\n",
      "piercing -> pierce | position: v\n",
      "rounded -> round | position: v\n",
      "extends -> extend | position: v\n",
      "groomed -> groom | position: v\n",
      "causing -> cause | position: v\n",
      "lower -> low | position: a\n",
      "tended -> tend | position: v\n",
      "styled -> style | position: v\n",
      "tending -> tend | position: v\n",
      "braided -> braid | position: v\n",
      "smiling -> smile | position: v\n",
      "seems -> seem | position: v\n",
      "wearing -> wear | position: v\n",
      "combed -> comb | position: v\n",
      "looking -> look | position: v\n",
      "wearing -> wear | position: v\n",
      "running -> run | position: v\n",
      "tied -> tie | position: v\n",
      "sized -> size | position: v\n",
      "seems -> seem | position: v\n",
      "styled -> style | position: v\n",
      "darkens -> darken | position: v\n",
      "sunken -> sink | position: v\n",
      "left -> leave | position: v\n",
      "seems -> seem | position: v\n",
      "pronounced -> pronounce | position: v\n",
      "layered -> layer | position: v\n",
      "looking -> look | position: v\n",
      "pulled -> pull | position: v\n",
      "balding -> bald | position: v\n",
      "built -> build | position: v\n",
      "looking -> look | position: v\n",
      "interesting -> interest | position: v\n",
      "left -> leave | position: v\n",
      "looking -> look | position: v\n",
      "challenged -> challenge | position: v\n",
      "pulled -> pull | position: v\n",
      "sized -> size | position: v\n",
      "extends -> extend | position: v\n",
      "sighted -> sight | position: v\n",
      "looking -> look | position: v\n",
      "matured -> mature | position: v\n",
      "maintains -> maintain | position: v\n",
      "appears -> appear | position: v\n",
      "detached -> detach | position: v\n",
      "pronounced -> pronounce | position: v\n",
      "bigger -> big | position: a\n",
      "including -> include | position: v\n",
      "waving -> wave | position: v\n",
      "done -> do | position: v\n",
      "tied -> tie | position: v\n",
      "graying -> gray | position: v\n",
      "shorter -> short | position: a\n",
      "aged -> age | position: v\n",
      "tired -> tire | position: v\n",
      "bigger -> big | position: a\n",
      "shaved -> shave | position: v\n",
      "obscures -> obscure | position: v\n",
      "joined -> join | position: v\n",
      "pulled -> pull | position: v\n",
      "wearing -> wear | position: v\n",
      "slicked -> slick | position: v\n",
      "arched -> arch | position: v\n",
      "pointed -> point | position: v\n",
      "rounded -> round | position: v\n",
      "aged -> age | position: v\n",
      "seems -> seem | position: v\n",
      "looking -> look | position: v\n",
      "grooming -> groom | position: v\n",
      "tied -> tie | position: v\n",
      "appears -> appear | position: v\n",
      "looking -> look | position: v\n",
      "faced -> face | position: v\n",
      "trimmed -> trim | position: v\n",
      "swept -> sweep | position: v\n",
      "freaking -> freak | position: v\n",
      "got -> get | position: v\n",
      "assuming -> assume | position: v\n",
      "flowing -> flow | position: v\n",
      "pierced -> pierce | position: v\n",
      "built -> build | position: v\n",
      "dyed -> dye | position: v\n",
      "wearing -> wear | position: v\n",
      "wearing -> wear | position: v\n",
      "tired -> tire | position: v\n",
      "pulled -> pull | position: v\n",
      "tanned -> tan | position: v\n",
      "appears -> appear | position: v\n",
      "considering -> consider | position: v\n",
      "toned -> tone | position: v\n",
      "tanned -> tan | position: v\n",
      "wearing -> wear | position: v\n",
      "receding -> recede | position: v\n",
      "looking -> look | position: v\n",
      "pierced -> pierce | position: v\n",
      "younger -> young | position: a\n",
      "pronounced -> pronounce | position: v\n",
      "heavier -> heavy | position: a\n",
      "appears -> appear | position: v\n",
      "skinned -> skin | position: v\n",
      "trimmed -> trim | position: v\n",
      "built -> build | position: v\n",
      "longer -> long | position: a\n",
      "appears -> appear | position: v\n",
      "appears -> appear | position: v\n",
      "appears -> appear | position: v\n",
      "appears -> appear | position: v\n",
      "pulled -> pull | position: v\n",
      "receding -> recede | position: v\n",
      "smiling -> smile | position: v\n",
      "seems -> seem | position: v\n",
      "looking -> look | position: v\n",
      "looking -> look | position: v\n",
      "tanned -> tan | position: v\n",
      "pronounced -> pronounce | position: v\n",
      "seems -> seem | position: v\n",
      "dyed -> dye | position: v\n",
      "shaped -> shape | position: v\n",
      "bridged -> bridge | position: v\n",
      "appears -> appear | position: v\n",
      "thinner -> thin | position: a\n",
      "apposed -> appose | position: v\n",
      "looking -> look | position: v\n",
      "pulled -> pull | position: v\n",
      "heavier -> heavy | position: a\n",
      "sunken -> sink | position: v\n",
      "seems -> seem | position: v\n",
      "seems -> seem | position: v\n",
      "tied -> tie | position: v\n",
      "bit -> bite | position: v\n",
      "shorter -> short | position: a\n",
      "tied -> tie | position: v\n",
      "pulled -> pull | position: v\n",
      "pissed -> piss | position: v\n",
      "seems -> seem | position: v\n",
      "appears -> appear | position: v\n",
      "brushed -> brush | position: v\n",
      "appears -> appear | position: v\n",
      "wearing -> wear | position: v\n",
      "seems -> seem | position: v\n",
      "weighs -> weigh | position: v\n",
      "seems -> seem | position: v\n",
      "larger -> large | position: a\n",
      "sized -> size | position: v\n",
      "rounded -> round | position: v\n",
      "pulled -> pull | position: v\n",
      "bit -> bite | position: v\n",
      "older -> old | position: a\n",
      "pulled -> pull | position: v\n",
      "pronounced -> pronounce | position: v\n",
      "slumped -> slump | position: v\n",
      "seen -> see | position: v\n",
      "shaped -> shape | position: v\n",
      "tied -> tie | position: v\n",
      "asking -> ask | position: v\n",
      "appears -> appear | position: v\n",
      "tied -> tie | position: v\n",
      "shaped -> shape | position: v\n",
      "parted -> part | position: v\n",
      "shorter -> short | position: a\n",
      "seems -> seem | position: v\n",
      "appears -> appear | position: v\n",
      "older -> old | position: a\n",
      "cleft -> cleave | position: v\n",
      "sized -> size | position: v\n",
      "mentioned -> mention | position: v\n",
      "attracts -> attract | position: v\n",
      "stressed -> stress | position: v\n",
      "going -> go | position: v\n",
      "died -> die | position: v\n",
      "cropped -> crop | position: v\n",
      "protruding -> protrude | position: v\n",
      "shorter -> short | position: a\n",
      "pulled -> pull | position: v\n",
      "pierced -> pierce | position: v\n",
      "appears -> appear | position: v\n",
      "rimmed -> rim | position: v\n",
      "colored -> color | position: v\n",
      "pulled -> pull | position: v\n",
      "left -> leave | position: v\n",
      "seems -> seem | position: v\n",
      "stretched -> stretch | position: v\n",
      "taller -> tall | position: a\n",
      "defining -> define | position: v\n",
      "arched -> arch | position: v\n",
      "curved -> curve | position: v\n",
      "made -> make | position: v\n",
      "looking -> look | position: v\n",
      "aged -> age | position: v\n",
      "wrinkled -> wrinkle | position: v\n",
      "looking -> look | position: v\n",
      "pointed -> point | position: v\n",
      "indicated -> indicate | position: v\n",
      "sloping -> slop | position: v\n",
      "aged -> age | position: v\n",
      "elongated -> elongate | position: v\n",
      "combed -> comb | position: v\n",
      "pointed -> point | position: v\n",
      "piercing -> pierce | position: v\n",
      "running -> run | position: v\n",
      "wearing -> wear | position: v\n",
      "appears -> appear | position: v\n",
      "shaped -> shape | position: v\n",
      "parted -> part | position: v\n",
      "marred -> mar | position: v\n",
      "done -> do | position: v\n",
      "seems -> seem | position: v\n",
      "built -> build | position: v\n",
      "wearing -> wear | position: v\n",
      "colored -> color | position: v\n",
      "seems -> seem | position: v\n",
      "receding -> recede | position: v\n",
      "wearing -> wear | position: v\n",
      "framed -> frame | position: v\n",
      "pulled -> pull | position: v\n",
      "wearing -> wear | position: v\n",
      "bigger -> big | position: a\n",
      "left -> leave | position: v\n",
      "wearing -> wear | position: v\n",
      "brushed -> brush | position: v\n",
      "shaped -> shape | position: v\n",
      "wearing -> wear | position: v\n",
      "printed -> print | position: v\n",
      "left -> leave | position: v\n",
      "tanned -> tan | position: v\n",
      "appears -> appear | position: v\n",
      "appears -> appear | position: v\n",
      "appears -> appear | position: v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marking -> mark | position: v\n",
      "appears -> appear | position: v\n",
      "appears -> appear | position: v\n",
      "older -> old | position: a\n",
      "appears -> appear | position: v\n",
      "piercing -> pierce | position: v\n",
      "saying -> say | position: v\n",
      "looking -> look | position: v\n",
      "pulled -> pull | position: v\n",
      "wearing -> wear | position: v\n",
      "looking -> look | position: v\n",
      "pulled -> pull | position: v\n",
      "groomed -> groom | position: v\n",
      "shaped -> shape | position: v\n",
      "aged -> age | position: v\n",
      "disheveled -> dishevel | position: v\n",
      "looking -> look | position: v\n",
      "balding -> bald | position: v\n",
      "shaved -> shave | position: v\n",
      "wearing -> wear | position: v\n",
      "pulled -> pull | position: v\n",
      "pressed -> press | position: v\n",
      "skinned -> skin | position: v\n",
      "breasted -> breast | position: v\n",
      "lacking -> lack | position: v\n",
      "wearing -> wear | position: v\n",
      "lacking -> lack | position: v\n",
      "proportioned -> proportion | position: v\n",
      "slanted -> slant | position: v\n",
      "styled -> style | position: v\n",
      "built -> build | position: v\n",
      "rose -> rise | position: v\n",
      "looking -> look | position: v\n",
      "appears -> appear | position: v\n",
      "pierced -> pierce | position: v\n",
      "loving -> love | position: v\n",
      "styling -> style | position: v\n",
      "kept -> keep | position: v\n",
      "pulled -> pull | position: v\n",
      "arched -> arch | position: v\n",
      "appears -> appear | position: v\n",
      "tied -> tie | position: v\n",
      "appears -> appear | position: v\n",
      "appearing -> appear | position: v\n",
      "seems -> seem | position: v\n",
      "trying -> try | position: v\n",
      "slicked -> slick | position: v\n",
      "appears -> appear | position: v\n",
      "wearing -> wear | position: v\n",
      "appears -> appear | position: v\n",
      "framed -> frame | position: v\n",
      "pointed -> point | position: v\n",
      "combed -> comb | position: v\n",
      "left -> leave | position: v\n",
      "aged -> age | position: v\n",
      "receding -> recede | position: v\n",
      "larger -> large | position: a\n",
      "bigger -> big | position: a\n",
      "seems -> seem | position: v\n",
      "shorter -> short | position: a\n",
      "wearing -> wear | position: v\n",
      "seems -> seem | position: v\n",
      "bridged -> bridge | position: v\n",
      "aged -> age | position: v\n",
      "shorter -> short | position: a\n",
      "looking -> look | position: v\n",
      "wearing -> wear | position: v\n",
      "smaller -> small | position: a\n",
      "looking -> look | position: v\n",
      "receding -> recede | position: v\n",
      "graying -> gray | position: v\n",
      "dyed -> dye | position: v\n",
      "seems -> seem | position: v\n",
      "manicured -> manicure | position: v\n",
      "colored -> color | position: v\n",
      "aged -> age | position: v\n",
      "shouldered -> shoulder | position: v\n",
      "brushed -> brush | position: v\n",
      "shaped -> shape | position: v\n",
      "skinnier -> skinny | position: a\n",
      "left -> leave | position: v\n",
      "shorter -> short | position: a\n",
      "looking -> look | position: v\n",
      "combed -> comb | position: v\n",
      "wearing -> wear | position: v\n",
      "colored -> color | position: v\n",
      "wearing -> wear | position: v\n",
      "left -> leave | position: v\n",
      "aged -> age | position: v\n"
     ]
    }
   ],
   "source": [
    "# Generating lemmatized sentences\n",
    "\n",
    "lemmatized_sentences = []\n",
    "lemmatized_words = []\n",
    "\n",
    "for each in range(len(tokenized_words)):\n",
    "    lemmatized_words = []\n",
    "    for i in range(len(tokenized_words[each])):\n",
    "        if tokenized_words[each][i][2] in [\"a\", \"n\", \"v\", \"r\"]: \n",
    "            if tokenized_words[each][i][0] != lemmatizer.lemmatize(tokenized_words[each][i][0], tokenized_words[each][i][2]):\n",
    "                print(tokenized_words[each][i][0], \"->\", lemmatizer.lemmatize(tokenized_words[each][i][0], tokenized_words[each][i][2]), \"| position:\", tokenized_words[each][i][2])                \n",
    "                lemmatized_words.append(lemmatizer.lemmatize(tokenized_words[each][i][0], tokenized_words[each][i][2]))\n",
    "            else:\n",
    "                lemmatized_words.append(tokenized_words[each][i][0])\n",
    "        else:\n",
    "            lemmatized_words.append(tokenized_words[each][i][0])\n",
    "    lemmatized_sentences.append(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " This person is a Caucasian male with brown hair and brown eyes.\n",
      "Optimized:\n",
      " person caucasian male brown hair brown eye\n",
      "Lemmatized:\n",
      " person caucasian male brown hair brown eye\n",
      "\n",
      "Original:\n",
      " A block face with a semi large nose. Short hair that is combed over. Somewhat large forehead.\n",
      "Optimized:\n",
      " block face semi large nose short hair combed somewhat large forehead\n",
      "Lemmatized:\n",
      " block face semi large nose short hair comb somewhat large forehead\n",
      "\n",
      "Original:\n",
      " this person appears to be a female in their late twenties. they have brown hair in a ponytail the length is somewhat hidden but looks to be a bit longer than shoulder length.  she has dark eyes brown hair somewhat thick arched eyebrows that are close to her eyes a slightly wide nose and average lips.  she has her ears pierced. her skin is fair and blemish-free.\n",
      "Optimized:\n",
      " person appears female late twenty they brown hair ponytail length somewhat hidden look bit longer shoulder length she dark eye brown hair somewhat thick arched eyebrow close her eye slightly wide nose average lip she her ear pierced her skin fair blemish free\n",
      "Lemmatized:\n",
      " person appear female late twenty they brown hair ponytail length somewhat hidden look bit longer shoulder length she dark eye brown hair somewhat thick arch eyebrow close her eye slightly wide nose average lip she her ear pierce her skin fair blemish free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test out \n",
    "for x in range(3):\n",
    "    i = np.random.randint(0, len(content))\n",
    "    print(\"Original:\\n\", content[i])\n",
    "    print(\"Optimized:\\n\", \" \".join(str(word) for word in words[i]))    \n",
    "    print(\"Lemmatized:\\n\", \" \".join(str(lemmatized_word) for lemmatized_word in lemmatized_sentences[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate overall word frequency distribution\n",
    "input_sentence = lemmatized_sentences\n",
    "\n",
    "lem_words = []\n",
    "for i in range(len(input_sentence)):\n",
    "    for each in input_sentence[i]:\n",
    "        lem_words.append(each)\n",
    "\n",
    "lem_freq_dist = nltk.FreqDist(lem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_index(text):\n",
    "    word_index = {}\n",
    "    index = 0\n",
    "    for line in text:\n",
    "        for word in line:\n",
    "            if not word in word_index:\n",
    "                word_index[word] = index\n",
    "                index += 1\n",
    "    return word_index\n",
    "\n",
    "def word_matrix_dan(text):\n",
    "    word_idx = word_index(text)\n",
    "    word_matrix = scipy.sparse.dok_matrix((len(text), len(word_idx)))\n",
    "    max_val = max(lem_freq_dist.values())\n",
    "    \n",
    "    doc_num = 0\n",
    "    for line in text:\n",
    "        for word in line:\n",
    "            word_num = word_idx[word]\n",
    "            # Original function:\n",
    "            # word_matrix[doc_num, word_num] += 1 \n",
    "            # Our replacement:\n",
    "            word_matrix[doc_num, word_num] += 1 * (np.log(max_val+1) - np.log(lem_freq_dist[word]+1))\n",
    "            # Rationale - we are looking for a weighing function to adjust the scale of each dictionary entry\n",
    "            # according to the word's overall frequency across the entire corpus. After testing TFID, which\n",
    "            # includes similar functionality, and achieving unsatisfactory results, we built our own formula. \n",
    "            # Additional formulae tested:\n",
    "            #word_matrix[doc_num, word_num] += 1 * (np.log(max_val) - np.log(freq_dist[word]))\n",
    "            #word_matrix[doc_num, word_num] = word_matrix[doc_num, word_num] / freq_dist[word]\n",
    "            #word_matrix[doc_num, word_num] = word_matrix[doc_num, word_num] * (np.log(max_val+1) - np.log(freq_dist[word]+1))\n",
    "        doc_num += 1        \n",
    "    return word_matrix\n",
    "\n",
    "doc_matrix = word_matrix_dan(input_sentence)\n",
    "\n",
    "# Bigram Threshold: 5.83\n",
    "final_matrix = doc_matrix.toarray()\n",
    "final_matrix[doc_matrix.toarray() > 5.83] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD Truncation\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components = 250, n_iter = 7, random_state = 42)\n",
    "\n",
    "tfidf_svd = svd.fit_transform(final_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "similarity = sklearn.metrics.pairwise.cosine_similarity(tfidf_svd)\n",
    "similarity = np.around(similarity, decimals=4)\n",
    "similarity[similarity == 1] = 0\n",
    "\n",
    "simi_list = []\n",
    "for i in range(772):\n",
    "    a, b, c = np.argsort(similarity[i])[-3:]\n",
    "    simi_list.append(a)\n",
    "    simi_list.append(b)\n",
    "    simi_list.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## JACCARD SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(str1, str2): \n",
    "    a = set(str1) \n",
    "    b = set(str2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacc_total=[]\n",
    "for i in range(len(lemmatized_sentences)):\n",
    "    jacc_score=[]\n",
    "    for j in range(len(lemmatized_sentences)):\n",
    "            jacc_score.append(get_jaccard_sim(lemmatized_sentences[i], lemmatized_sentences[j]))\n",
    "    jacc_total.append(jacc_score)\n",
    "\n",
    "#jacc_total can be passed to the output\n",
    "similarity= np.array(jacc_total)\n",
    "\n",
    "similarity = np.around(similarity, decimals=4)\n",
    "similarity[similarity == 1] = 0\n",
    "\n",
    "simi_list = []\n",
    "for i in range(772):\n",
    "    a, b, c = np.argsort(similarity[i])[-3:]\n",
    "    simi_list.append(a)\n",
    "    simi_list.append(b)\n",
    "    simi_list.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRELATION DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "doc_corr_matrix = word_matrix_dan(lemmatized_sentences)\n",
    "doc_corr_matrix= doc_corr_matrix.toarray()\n",
    "corr_total=[]\n",
    "for i in range(len(doc_corr_matrix)):\n",
    "    corr_score=[]\n",
    "    for j in range(len(doc_corr_matrix)):\n",
    "            corr_score.append(distance.correlation(doc_corr_matrix[i], doc_corr_matrix[j]))\n",
    "    corr_total.append(corr_score)\n",
    "    \n",
    "similarity= np.array(corr_total)\n",
    "\n",
    "similarity = np.around(similarity, decimals=4)\n",
    "similarity[similarity == 1] = 0\n",
    "\n",
    "simi_list = []\n",
    "for i in range(772):\n",
    "    a, b, c = np.argsort(similarity[i])[-3:]\n",
    "    simi_list.append(a)\n",
    "    simi_list.append(b)\n",
    "    simi_list.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating submission file\n",
    "\n",
    "simi_list = np.array(simi_list).reshape(772, 3)\n",
    "baseline = np.arange(0, 772).reshape(772, 1)\n",
    "submission_matrix = np.hstack((baseline, simi_list))\n",
    "np.savetxt(\"submission.txt\", submission_matrix, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approaches: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generating N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = lemmatized_sentences\n",
    "\n",
    "bigrams = []\n",
    "for sentence in sentences:\n",
    "    sequence = word_tokenize(\" \".join([e for e in sentence])) \n",
    "    bigrams.extend(list(ngrams(sequence, 2)))\n",
    "\n",
    "freq_dist = nltk.FreqDist(bigrams)\n",
    "prob_dist = nltk.MLEProbDist(freq_dist)\n",
    "number_of_bigrams = freq_dist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Frequency distribution of top 20 bigrams\n",
    "plt.figure(figsize=(14,10))\n",
    "freq_dist.plot(20,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beautiful': 112, 'eyebrow': 357, 'nice': 678, 'hair': 455, 'cut': 260, 'his': 497, 'complexion': 225, 'clean': 203, 'young': 1100, 'look': 591, 'ear': 317, 'big': 120, 'care': 173, 'although': 61, 'people': 725, 'probably': 773, 'make': 604, 'fun': 415, 'unfortunately': 1036, 'they': 986, 'short': 869, 'dark': 263, 'light': 575, 'brown': 155, 'olive': 698, 'skin': 887, 'thick': 987, 'red': 807, 'lip': 582, 'patchy': 721, 'facial': 365, 'wear': 1065, 'large': 554, 'frame': 400, 'sign': 876, 'acne': 45, 'asian': 85, 'guy': 453, '20': 24, 'he': 471, 'tone': 1007, 'monolid': 644, 'eye': 356, 'small': 906, 'body': 140, 'build': 160, 'like': 579, 'would': 1091, 'straight': 946, 'person': 730, 'crew': 248, 'dirty': 291, 'blonde': 133, 'squarish': 931, 'face': 363, 'round': 830, 'jaw': 535, 'skinny': 889, 'average': 97, 'weight': 1068, 'long': 588, 'comb': 218, 'emo': 334, 'beard': 111, 'scraggily': 844, 'not': 683, 'well': 1070, 'keep': 543, 'she': 862, 'slightly': 898, 'black': 125, 'shirt': 865, 'new': 676, 'knowledge': 549, 'stature': 940, 'thin': 990, 'casual': 175, 'pull': 788, 'back': 100, 'neat': 666, 'high': 491, 'deep': 271, 'set': 851, 'extreme': 352, 'laugh': 557, 'line': 581, 'wrinkle': 1092, 'unpleasant': 1042, 'attitude': 89, 'fair': 369, 'attractive': 91, 'open': 700, 'judge': 541, 'style': 960, 'great': 442, 'seem': 848, 'due': 313, 'also': 60, 'general': 423, 'unhappy': 1037, 'teen': 981, 'expression': 348, 'appear': 73, 'groom': 446, 'current': 257, 'mid': 629, 'length': 567, 'unkempt': 1041, 'mole': 643, 'top': 1009, 'leave': 564, 'cheek': 187, 'flat': 387, 'nose': 681, 'serious': 850, 'bite': 123, 'stubble': 955, 'left': 565, 'stick': 942, 'curl': 254, 'white': 1075, 'male': 606, 'color': 217, 'mustache': 657, 'say': 839, 'letter': 570, 'oval': 710, 'shape': 856, 'head': 472, 'medium': 622, 'size': 885, 'full': 413, 'around': 83, 'twenty': 1022, 'six': 883, 'year': 1096, 'age': 54, 'height': 483, 'birth': 121, 'mark': 614, 'her': 485, 'side': 873, 'slender': 893, 'never': 675, 'smile': 909, 'hygienic': 508, 'concern': 228, 'much': 650, 'shaven': 861, 'organize': 706, 'neck': 669, 'thirty': 995, 'five': 385, 'old': 696, 'wide': 1076, 'female': 377, 'think': 992, 'might': 633, 'either': 331, 'korean': 550, 'filipino': 380, 'scar': 840, 'man': 607, 'tall': 976, 'built': 161, 'curly': 256, 'pretty': 771, 'muscular': 653, 'smart': 908, 'college': 216, 'degree': 274, 'youthful': 1101, 'relatively': 814, 'happy': 467, 'creative': 247, 'observe': 688, 'thing': 991, 'always': 62, 'way': 1063, 'solid': 916, 'career': 174, 'bright': 151, 'future': 417, 'single': 880, 'casually': 176, 'date': 267, 'woman': 1084, 'headed': 474, 'girl': 428, 'wavy': 1062, 'towards': 1014, 'end': 335, 'forehead': 395, 'peak': 723, 'lean': 561, 'toward': 1013, 'cheekbone': 188, 'visible': 1056, 'prominent': 778, 'square': 930, 'caucasian': 177, 'attractiveness': 93, 'overweight': 713, 'slim': 900, 'type': 1024, 'require': 816, 'glass': 430, 'daily': 262, 'use': 1051, 'chin': 191, 'pierce': 743, 'tie': 997, 'ponytail': 761, 'broad': 152, 'shoulder': 871, 'goatee': 433, 'right': 818, 'soft': 915, 'feature': 376, 'grow': 449, 'draw': 306, 'focus': 392, 'away': 98, 'pony': 760, 'tail': 974, 'bad': 102, 'shaved': 860, 'bet': 118, 'good': 436, 'drop': 312, 'mixed': 639, 'trim': 1017, 'tan': 978, 'blue': 137, 'slight': 896, 'tattoo': 979, 'piercings': 745, 'green': 443, 'kind': 545, 'ton': 1006, 'class': 202, 'strong': 953, 'late': 555, 'latino': 556, 'maybe': 619, 'typical': 1025, 'little': 587, 'obese': 685, 'yet': 1099, 'athletic': 88, 'pale': 715, 'sckin': 842, 'piece': 742, 'identify': 511, 'pleasant': 754, 'bit': 122, 'hobble': 499, 'walk': 1059, 'play': 753, 'video': 1054, 'game': 419, 'suffers': 965, 'lack': 551, 'exercise': 347, 'middle': 630, 'moderately': 641, 'mostly': 645, 'blond': 132, 'root': 825, 'proportionate': 783, 'early': 320, 'bang': 108, 'sweep': 971, 'one': 699, 'somewhat': 921, 'curve': 258, 'spend': 926, 'lot': 596, 'time': 999, 'longer': 589, 'adult': 51, 'reddish': 808, 'taller': 977, 'hunch': 507, 'stance': 934, 'messy': 628, 'odd': 691, 'haircut': 457, 'patch': 720, 'strand': 947, 'orange': 703, 'chinstrap': 194, 'turn': 1021, 'inch': 517, 'two': 1023, 'day': 268, 'physical': 738, 'earsand': 322, 'point': 757, '130': 9, 'pound': 768, 'estimate': 340, 'stand': 935, 'foot': 394, 'dimple': 289, 'upto': 1048, 'circular': 201, 'clear': 204, 'structure': 954, 'maintain': 602, 'oclock': 690, 'shadow': 852, 'give': 429, 'appearance': 74, 'manliness': 611, 'touch': 1011, 'sun': 966, 'normal': 680, 'basic': 109, 'easily': 323, 'forgetable': 396, 'match': 616, 'hidden': 488, 'arch': 80, 'close': 210, 'blemish': 129, 'free': 404, 'intimidate': 530, 'graphic': 439, 'tee': 980, 'unique': 1039, 'bone': 141, 'chubby': 198, 'unfit': 1035, 'fit': 384, 'direction': 290, 'job': 538, 'active': 47, 'lift': 574, 'regularly': 812, 'bottom': 143, 'knob': 546, 'mouth': 648, 'lightly': 577, 'pointy': 758, 'fat': 374, 'thinner': 994, 'crease': 245, 'beside': 116, 'stringy': 952, 'droopy': 311, 'depress': 277, 'layer': 558, 'kept': 544, 'minimal': 636, 'noticeable': 684, 'makeup': 605, 'sized': 886, 'narrow': 658, 'bridge': 150, 'nostril': 682, 'jawline': 536, 'physically': 739, 'hispanic': 498, 'heritage': 486, 'closely': 211, 'crop': 250, 'rectangular': 806, 'sad': 836, 'nature': 664, 'homemaker': 502, 'lanky': 553, 'muscle': 652, 'proportioned': 784, 'extremely': 353, 'finger': 382, 'extremity': 354, 'descent': 278, 'fairly': 370, 'wire': 1082, 'intense': 526, 'impressive': 515, 'pettie': 733, 'haas': 454, 'longish': 590, 'though': 996, 'really': 802, 'shave': 859, 'real': 801, 'lopsided': 593, 'bun': 164, 'perhaps': 728, 'half': 462, 'bag': 103, 'rest': 817, 'frown': 410, '140': 13, 'earring': 321, '110': 3, 'lb': 559, 'petite': 732, 'shoe': 868, 'braid': 147, 'squinty': 933, 'diamond': 285, 'ruddy': 833, 'arab': 79, 'piss': 751, 'yellow': 1097, 'hijab': 492, 'common': 222, 'approximately': 77, '21': 27, 'moustache': 646, 'stocky': 945, 'weigh': 1066, '150': 16, '160': 17, 'midlength': 632, 'put': 792, '18ish': 22, 'tire': 1002, 'fuller': 414, '120': 6, 'greasy': 441, 'hoop': 503, 'semi': 849, '17': 18, 'someone': 918, 'animal': 69, 'environment': 339, 'american': 64, 'part': 716, 'width': 1080, 'could': 237, 'possible': 765, 'hazel': 470, 'shit': 866, 'many': 612, 'word': 1086, 'need': 671, 'evident': 344, 'balding': 106, 'drape': 305, 'corner': 236, 'fu': 412, 'manchu': 609, 'least': 563, 'bushy': 165, 'entirely': 338, 'chinese': 193, 'cute': 261, '30': 35, 'tshirt': 1020, 'sort': 922, 'ft': 411, '11': 2, 'weighs': 1067, 'delicate': 275, 'arm': 82, 'hand': 464, 'le': 560, 'hairy': 461, 'likely': 580, 'often': 693, 'droop': 310, 'thickness': 989, 'reasonably': 803, 'hyper': 509, 'mild': 634, 'distinguishable': 298, '100': 1, 'sharp': 858, 'upper': 1047, 'stock': 944, '190': 23, 'east': 324, 'opinion': 701, 'phot': 734, 'camera': 170, 'blaank': 124, 'stare': 937, 'shock': 867, 'photo': 735, 'vibrant': 1053, 'rather': 800, 'disposition': 296, 'ethnicity': 341, 'haired': 458, 'eyed': 358, '25': 31, '40': 36, 'hard': 468, 'tell': 982, 'worth': 1089, 'growth': 451, 'rat': 798, 'bulbous': 162, 'near': 665, 'angry': 68, 'feminine': 379, 'cleft': 206, 'sleeve': 892, 'life': 573, 'impact': 514, 'stylish': 962, 'hairstyle': 460, 'bland': 127, 'wave': 1061, 'grown': 450, 'mustace': 656, 'scruff': 845, 'crook': 249, 'frizzy': 408, 'atypical': 95, 'fatty': 375, 'decent': 269, 'golden': 435, 'irish': 532, 'gauge': 420, 'mind': 635, 'get': 427, 'know': 548, 'detect': 283, 'posture': 767, 'maybe6': 620, 'prominant': 777, '28': 33, 'grey': 445, 'define': 272, 'hairline': 459, 'text': 985, 'chisel': 195, 'neatly': 667, 'circle': 200, 'nordid': 679, 'disheveled': 294, 'completely': 224, 'alert': 56, 'extreamly': 351, 'sandy': 837, 'gel': 422, 'slouch': 903, 'across': 46, 'come': 219, 'low': 599, 'hide': 489, 'work': 1087, 'smooth': 912, 'flip': 388, 'front': 409, 'eyelid': 360, 'plain': 752, 'clothing': 213, 'partially': 717, 'almond': 57, 'heightgood': 484, 'stylesmall': 961, 'rosy': 827, 'freckle': 403, 'heavy': 480, 'butch': 166, 'strawberry': 948, 'darker': 265, 'india': 519, 'south': 924, 'america': 63, 'dyed': 316, 'brownish': 156, 'indian': 520, 'harir': 469, '41': 37, 'sparse': 925, 'firm': 383, 'clock': 209, 'inflated': 523, 'protruding': 786, 'outwards': 709, 'purse': 791, 'greenish': 444, 'sweater': 970, 'underneath': 1031, 'something': 919, 'write': 1093, '10': 0, '170': 19, 'couple': 239, 'healthy': 477, 'masculine': 615, 'earings': 318, 'femenin': 378, 'egg': 330, 'upturned': 1049, 'dress': 307, 'simple': 878, 'hairbroad': 456, 'pronounce': 780, 'aqua': 78, 'possibly': 766, 'relative': 813, 'go': 432, 'pronouced': 779, 'median': 621, 'range': 797, 'sixty': 884, 'french': 405, 'gray': 440, 'bald': 105, 'scowl': 843, 'comfortable': 220, 'sport': 928, 'piglike': 746, 'groomed': 447, 'flush': 391, 'despite': 281, 'cropped': 251, 'tiny': 1000, 'heart': 478, 'rough': 829, 'even': 342, 'blotch': 135, 'fringe': 407, 'sturdily': 959, 'start': 938, 'ring': 821, '26': 32, 'clothes': 212, 'mention': 626, 'quote': 795, 'disable': 292, 'yes': 1098, 'tired': 1003, 'blackish': 126, 'whispy': 1074, 'darkstraightbrown': 266, 'eybrows': 355, 'auburn': 96, 'scruffy': 846, 'angle': 67, 'hipster': 496, 'gentle': 426, 'others': 707, 'may': 618, 'rude': 834, 'topmost': 1010, 'dye': 315, 'neutral': 674, 'african': 52, 'standard': 936, 'unsure': 1046, 'whether': 1073, 'leaning': 562, 'scarf': 841, 'slime': 901, 'friendly': 406, 'guess': 452, 'sure': 969, 'older': 697, 'messiness': 627, 'intentional': 527, 'john': 539, 'hamm': 463, 'baby': 99, 'driver': 309, 'pair': 714, 'national': 660, '175': 20, 'professional': 776, '6ft': 42, 'per': 726, 'view': 1055, 'carable': 172, 'funny': 416, 'gage': 418, 'almost': 58, 'terrible': 984, 'smallish': 907, 'ideal': 510, 'bore': 142, 'individual': 522, 'slighty': 899, 'brunette': 158, 'picture': 741, 'stylize': 963, 'minimalist': 637, 'quite': 794, 'rim': 819, 'cover': 241, 'shorter': 870, 'traditional': 1015, 'maintenance': 603, 'baggy': 104, 'block': 131, 'forward': 399, 'fashionable': 373, 'heavyset': 481, 'chunky': 199, 'symmetrical': 973, 'along': 59, 'additionally': 50, 'breast': 148, 'try': 1019, 'extend': 349, 'oblong': 686, 'uneven': 1034, 'cause': 178, 'handsome': 465, 'charasmatic': 185, 'thicker': 988, 'lighter': 576, 'pock': 756, 'tend': 983, 'unremarkable': 1043, 'let': 569, 'save': 838, 'develope': 284, 'take': 975, 'shaped': 857, 'complexity': 226, 'brow': 154, 'pink': 749, 'neon': 673, 'streak': 949, 'run': 835, 'behind': 113, 'amount': 65, 'bob': 139, 'concealer': 227, 'darken': 264, 'lipstick': 585, 'still': 943, 'distinct': 297, 'sink': 881, 'spot': 929, 'oddly': 692, 'lumpy': 600, 'gorgeous': 437, 'pearl': 724, 'condition': 229, 'lose': 594, 'cancer': 171, 'patient': 722, 'wake': 1058, 'idiot': 512, 'stern': 941, 'junior': 542, '5foot': 40, 'interest': 529, 'energetic': 336, 'overall': 711, 'rosey': 826, 'challenge': 182, 'leg': 566, 'ordinary': 704, 'zit': 1103, '20ish': 26, 'blondish': 134, 'roundish': 832, 'past': 719, 'proportion': 781, 'another': 70, 'race': 796, 'sight': 875, 'mature': 617, 'must': 655, 'manager': 608, 'organization': 705, '24': 29, '135': 11, 'weak': 1064, '15': 15, 'yr': 1102, 'detach': 282, 'earlobe': 319, 'pinkish': 750, 'include': 518, 'do': 299, 'native': 661, 'since': 879, 'option': 702, 'chose': 196, 'exceedingly': 346, 'ugly': 1027, 'knot': 547, 'group': 448, 'spiky': 927, 'country': 238, 'unattractive': 1029, '180': 21, 'student': 957, 'edge': 327, 'graduation': 438, 'upwards': 1050, 'obscure': 687, 'lib': 571, 'tightly': 998, 'join': 540, 'skull': 890, 'eastern': 325, 'culture': 253, 'oversized': 712, 'arge': 81, 'decently': 270, '120lbs': 7, 'slick': 894, 'inbred': 516, 'faggot': 367, 'nasty': 659, 'defined': 273, 'pimply': 748, 'skinned': 888, 'approx': 76, 'without': 1083, 'mush': 654, 'effort': 329, 'character': 183, 'lovable': 597, 'socialize': 914, 'forty': 398, 'facetanned': 364, 'homely': 501, 'toned': 1008, 'hippie': 495, 'band': 107, 'super': 967, 'freak': 402, 'hot': 505, 'holy': 500, 'crap': 242, 'smokey': 911, 'assume': 87, 'flow': 389, 'attractivefair': 92, 'lipshas': 583, 'charming': 186, 'personality': 731, 'natural': 662, 'looker': 592, 'gold': 434, 'headdress': 473, '125': 8, 'faint': 368, 'consider': 234, 'connect': 232, 'sideburn': 874, 'necessity': 668, 'recede': 804, 'hiar': 487, 'pouty': 769, 'shes': 863, 'bruise': 157, 'clip': 208, 'hint': 493, 'an': 66, 'pimple': 747, 'confident': 230, 'rate': 799, 'proportional': 782, 'uncoordinated': 1030, 'volleyball': 1057, 'unable': 1028, 'move': 649, 'effectively': 328, 'puppy': 789, 'intelligent': 525, 'tough': 1012, 'squat': 932, '130lbs': 10, 'ash': 84, 'mohawk': 642, 'appose': 75, 'western': 1071, 'slience': 895, 'moderate': 640, 'boyish': 146, 'studious': 958, 'womanslightly': 1085, 'thinker': 993, 'worker': 1088, 'role': 823, 'slighter': 897, 'shiny': 864, 'eyeshadow': 361, 'gloss': 431, 'polite': 759, 'sociable': 913, 'japanese': 534, 'brush': 159, 'however': 506, 'addition': 49, 'together': 1004, 'create': 246, 'otherwise': 708, '115': 4, 'widow': 1079, 'fine': 381, 'love': 598, 'sould': 923, 'fix': 386, 'cobby': 214, 'tip': 1001, 'generally': 424, 'double': 301, 'fold': 393, 'slump': 905, 'dresser': 308, 'trendy': 1016, 'design': 280, 'see': 847, 'plump': 755, 'bulky': 163, 'somber': 917, 'demeanor': 276, 'sit': 882, 'room': 824, 'sometimes': 920, 'chess': 189, 'shallow': 854, 'description': 279, 'enough': 337, 'box': 144, 'ask': 86, 'understand': 1032, 'heavily': 479, 'widish': 1078, 'receding': 805, 'supervisor': 968, 'factory': 366, 'loss': 595, 'madurai': 601, 'pas': 718, 'curley': 255, 'confuse': 231, 'mental': 624, 'disorder': 295, 'cleave': 205, 'rounded': 831, 'center': 179, 'subjective': 964, 'course': 240, 'positive': 764, 'challanged': 181, 'mentally': 625, 'smiley': 910, 'attract': 90, 'stress': 950, 'intimidating': 531, 'hefty': 482, '70': 43, '73': 44, '200': 25, '240': 30, 'dim': 288, 'headful': 475, 'best': 117, 'problem': 774, 'physic': 737, 'easy': 326, 'die': 286, 'curvy': 259, 'liips': 578, 'protrude': 785, 'buzz': 167, '23': 28, 'hiding': 490, 'complection': 223, 'purple': 790, 'creamy': 244, 'conservative': 233, 'unshaven': 1045, 'piercing': 744, 'middleage': 631, 'stretch': 951, 'characteristic': 184, 'buzzcut': 168, 'downward': 303, 'perfect': 727, 'chain': 180, 'necklace': 670, 'dumpy': 314, 'meh': 623, 'boy': 145, 'bend': 115, 'poor': 762, 'eyesight': 362, 'lid': 572, 'indicate': 521, 'slop': 902, 'elongate': 332, 'horseshoe': 504, 'rimmed': 820, 'bronze': 153, 'product': 775, 'actually': 48, 'belong': 114, 'mar': 613, 'neither': 672, 'underweight': 1033, 'contrary': 235, 'portion': 763, 'statement': 939, 'mousy': 647, 'china': 192, 'everything': 343, 'silhouette': 877, 'geek': 421, 'region': 811, 'afro': 53, 'phrase': 736, 'print': 772, 'multiple': 651, 'bmi': 138, 'obvious': 689, 'blank': 128, 'quiet': 793, 'relaxed': 815, 'puffy': 787, 'oily': 694, 'ok': 695, 'lenth': 568, 'jane': 533, 'doent': 300, 'peron': 729, 'attracts': 94, 'lady': 552, '65': 41, 'redish': 809, 'slow': 904, 'doughy': 302, 'illness': 513, 'unhealthy': 1038, 'xl': 1094, 'christian': 197, 'else': 333, 'different': 287, 'wind': 1081, 'blown': 136, 'whatsoever': 1072, 'warm': 1060, 'redness': 810, '45': 38, '50': 39, 'rotund': 828, 'dishevel': 293, 'university': 1040, 'usual': 1052, 'hip': 494, 'generic': 425, 'press': 770, 'pick': 740, 'crowd': 252, 'lipsround': 584, 'stubbled': 956, 'fluffy': 390, 'trimmed': 1018, 'typically': 1026, 'slant': 891, 'rise': 822, 'lite': 586, 'exactly': 345, 'shaggy': 853, '145': 14, 'formal': 397, 'weird': 1069, 'extra': 350, 'togethere': 1005, 'cold': 215, 'show': 872, 'bias': 119, 'inscrutable': 524, 'background': 101, 'beak': 110, 'frazzled': 401, 'chestnut': 190, 'eyeglass': 359, 'fashion': 372, 'dramatic': 304, 'jib': 537, 'interaction': 528, 'mix': 638, 'unshaved': 1044, 'shampoo': 855, 'commercial': 221, 'worthy': 1090, 'aggressive': 55, '14': 12, '11ish': 5, 'anywhere': 71, 'handsomehis': 466, 'clefted': 207, 'breasted': 149, 'yard': 1095, 'health': 476, 'manicure': 610, 'calm': 169, 'blister': 130, 'naturally': 663, 'wider': 1077, 'farther': 371, 'apart': 72, '2ish': 34, 'next': 677, 'crazy': 243, 'swept': 972}\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for each in range(len(lemmatized_sentences)):\n",
    "    words.append(\" \".join([e for e in lemmatized_sentences[each]]))\n",
    "    \n",
    "vectorizer = CountVectorizer()\n",
    "text_features = vectorizer.fit_transform(words).todense()\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions arrays\n",
    "preds_bow = []\n",
    "preds_bow_final = []\n",
    "for each in range(len(text_features)):\n",
    "    preds_bow.append([each])\n",
    "    preds_bow_final.append([each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append reformatted pairwise distances to each element\n",
    "for each in range(len(text_features)):\n",
    "    for f in text_features:\n",
    "        preds_bow[each].append(float(str(euclidean_distances(text_features[each], f).flatten()).strip(\"[\").strip(\"]\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all but the 3 closest elements\n",
    "for each in range(len(preds_bow)):\n",
    "    for i in range(3):\n",
    "        preds_bow[each][np.argmin(preds_bow[each])] = 100000\n",
    "        preds_bow_final[each].append(np.argmin(preds_bow[each]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[660, 388, 590, 21],\n",
       " [741, 432, 256, 727],\n",
       " [463, 335, 388, 614],\n",
       " [230, 578, 674, 160],\n",
       " [333, 190, 266, 442]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at 5 random preds\n",
    "list(preds_bow_final[i] for i in [np.random.randint(len(preds_bow_final)) for n in range(5)])\n",
    "# Ready for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics: TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tfid = []\n",
    "for each in range(len(lemmatized_sentences)):\n",
    "    words_tfid.append(\" \".join([e for e in lemmatized_sentences[each]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform(words_tfid)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "\n",
    "pw_array = pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions array\n",
    "preds_tfid = []\n",
    "for each in range(len(pw_array)):\n",
    "    preds_tfid.append([each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in range(len(pw_array)):\n",
    "    for i in range(3):\n",
    "        pw_array[each][np.argmax(pw_array[each])] = 0\n",
    "    #print(\"#\", np.argmax(pw_array[each]), np.round(pw_array[each][np.argmax(pw_array[each])], 3))\n",
    "        preds_tfid[each].append(np.argmax(pw_array[each]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[727, 476, 368, 127],\n",
       " [264, 413, 762, 739],\n",
       " [550, 282, 565, 263],\n",
       " [592, 51, 285, 238],\n",
       " [347, 395, 205, 620]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at 5 random preds\n",
    "list(preds_tfid[i] for i in [np.random.randint(len(preds_tfid)) for n in range(5)])\n",
    "# Ready for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import coco_val data \n",
    "with open(\"coco_val.txt\") as f:\n",
    "    coco = f.readlines()\n",
    "    \n",
    "for each in range(len(coco)):\n",
    "    coco[each] = coco[each].strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 7180)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(coco)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = []\n",
    "c = 0\n",
    "for each in range(0, X_train_counts.shape[0], 5):\n",
    "    for i in range(5):\n",
    "        bayes.append(c)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 7180)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 7180)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = coco\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90268"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted[predicted == bayes]) / len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = sentences\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2378, 1775, 4248, 4248, 1765,  760, 3434, 3020, 1480,  760, 3434,\n",
       "        585, 2307, 2674, 2373, 2193,   80,  585, 3409,  743, 4248, 4181,\n",
       "        119, 1942, 2674, 2307, 1942, 2373, 2373, 1765, 1942, 3219, 1942,\n",
       "        623, 2373, 3409, 4248,  760,  760, 2373, 3409, 1942,  760, 4248,\n",
       "       1410,  220,  760, 2373,  760, 4248, 1410, 3409, 1942, 4399, 4865,\n",
       "       4248, 3409, 2373,  743, 2674, 1775,  760, 3020,  585, 1765, 1410,\n",
       "       4352,  760, 2625, 3344, 4671, 1942, 1765,  585, 4248, 3344, 3020,\n",
       "       1410, 1410,  760, 2373, 3344, 3434, 2865,  460, 2896, 3024, 4166,\n",
       "       3409, 4248, 3434, 1775, 3020, 3434, 2373, 4248,  760, 3480, 3020,\n",
       "       2094, 2373, 2294, 2373, 3020, 2611, 2289, 1410, 2373,  760, 1410,\n",
       "        760,   80, 1774, 2373, 2082, 3480, 1807, 4859, 2373,  760, 2373,\n",
       "       2373, 1765, 3480, 2611, 3431, 2373, 2373, 2373, 1647,  585,  760,\n",
       "        760, 3219, 4865,  583,   80, 3409, 3434, 3020, 2674, 1765,  760,\n",
       "        760, 2373, 1410,  743,  760, 3434,  760, 3020, 2373, 4753,  760,\n",
       "       1450, 1942,  963, 2165, 3409, 1775, 3434, 4248, 1641, 3020, 3857,\n",
       "       1942, 1410, 3547, 1410,  760, 2498, 3409, 4248, 2498, 3996, 4865,\n",
       "       1807, 2453,  760, 4865, 2686, 1410, 4865, 4248, 3975, 3434,  585,\n",
       "        743, 3020,  760, 3409,  760,  402, 4306, 1942, 3434, 3409,  760,\n",
       "        760,  760,  760, 2373, 3020,  760, 2373, 2515, 1281, 1410,   48,\n",
       "       1942, 1659,  760,  583,  872, 2611, 2373, 3434, 1775, 1410,  760,\n",
       "        760,  760,  743,  760, 3020, 1775, 4248, 1942, 3409,  760, 1018,\n",
       "        760, 1450, 3409, 3074, 4342, 3409, 3409, 3409, 2431, 3434, 4045,\n",
       "       2826, 3409, 3020, 1942, 2373, 1410,  743, 4248, 3344, 2373, 3409,\n",
       "       3434, 2878,  760,  760, 1942,  760,   80, 4865,  760, 1942,  743,\n",
       "       3364,  743, 3547,  201,  460, 1775, 2516, 4181, 2373,  760, 2227,\n",
       "       3434,  743,  815,  623, 1410, 3409, 1942, 2373, 2646, 2373, 3409,\n",
       "        583,  760,  386, 4248, 2373, 2373, 3547, 1765, 2373, 1769,  760,\n",
       "        381, 1765, 2227, 1410, 3480, 3434, 4890, 2896, 2515, 4045, 2373,\n",
       "       3020, 3409, 1410, 3409, 2431, 3020, 3364,  743, 2611,  760, 4428,\n",
       "       4865, 3434, 3219, 3741,  760,  583, 3434, 3547, 1410, 2826, 3020,\n",
       "       3480,   78, 3434,  760, 4248, 2373, 1769, 3996, 1450,  760,  743,\n",
       "       4045,  743, 2373, 1823, 4248, 4045,  497, 3020, 4248, 2307, 1067,\n",
       "       2674, 4248, 1892, 1410, 3434,  760, 4253, 2431, 4905, 3020, 3434,\n",
       "        760, 1410,  760,   94,  760, 3409, 1410,  760, 1765, 3344, 3074,\n",
       "       3683, 1410, 1765,  760,  743, 2515,  998, 3409, 1410, 3434, 2373,\n",
       "        760, 2108, 4248, 3020, 2373, 3020, 3409, 2373,  583, 1765, 1410,\n",
       "       2674, 4865,  760, 3302, 3480, 3409, 3364, 3434, 2373, 1769,  760,\n",
       "       2373,  760, 1823, 3409, 2865, 3409,  760, 2373, 3020, 2453, 1765,\n",
       "       1942, 4428, 3364,  760, 3434, 4248, 1942, 2373,  760, 3219, 1774,\n",
       "        743, 2373, 3409, 4248, 3409, 1410,   94, 3409,  760, 2373, 2307,\n",
       "         78,  760, 2373, 2373, 3020,  583, 4248, 1942,  760, 2373,   80,\n",
       "       2373, 1942,  760, 4520, 4248, 1775,  760,  760, 3409, 3434,  760,\n",
       "       2373, 1410, 1823,  693, 3538, 2686, 1942,  213,  583, 4248,  743,\n",
       "        760,  760, 1769, 2373,  760, 1765, 3125,  743,  760, 2373, 3409,\n",
       "       2686, 1410,  760, 3409,   78, 3020, 1765, 2515, 1410,  521, 1765,\n",
       "       1769, 2373, 3434, 3434, 1410, 4248,  760, 1942, 2373,  760, 2373,\n",
       "       3434, 3409, 3409, 3344, 1167, 3409, 3547, 4865, 2373, 3219, 3843,\n",
       "       3434, 3434, 1410, 2426, 1410, 3399,  760, 1410, 2373, 3409, 4045,\n",
       "       3409, 3434, 1410, 4181, 2373, 1069, 3409, 3219, 3344,  585, 1410,\n",
       "       3409,  760, 3020, 4248, 4248,  760, 3434, 2378, 3409,  760,  760,\n",
       "       2373, 1410,   76, 3987, 2373, 1774,  760, 4253, 1084, 1410, 2373,\n",
       "       2654, 1410, 3434,  760, 3434,  760,  583, 3020, 3409, 4663, 3409,\n",
       "       3834, 3364, 3975, 1769,  760,  760, 3434, 4181, 3409, 1480,  760,\n",
       "       1410,  569, 4168, 3434, 2373, 2373, 2373, 3219,  569, 4045, 3434,\n",
       "       4248, 3547, 4949, 2373, 2373, 2865, 3344, 1765,  945, 1410, 2373,\n",
       "        348,  760, 1769, 3928, 1410, 2469, 2519,  760, 1765, 1647, 3225,\n",
       "        623, 1942,  585,  585, 1769, 1769, 3434, 2826, 1410, 3434, 1765,\n",
       "       4248, 2082, 2878, 2373, 2373, 2896, 2082, 1765,  760, 3547,  760,\n",
       "        760,  743, 2373,  743, 1410,  583, 3344, 2923, 3409,  743, 1410,\n",
       "        760, 1769, 1765, 3434,  760, 3024, 1942,  760, 2373, 1769,  525,\n",
       "       4865, 1942, 1769, 1031,  760, 4248, 1456, 1410, 3219, 1769,  520,\n",
       "       2373,  760, 1456,  585,  760, 1991, 3074, 2373,  477, 1942,  525,\n",
       "       4865, 4556, 4865,  585,  815, 1250,  743, 1410,  760, 4865, 3818,\n",
       "       3219, 3409, 2373, 1410, 3344, 2373,    2, 3434,  760,  760, 4865,\n",
       "       2453, 3409, 1942, 2373, 4039, 2378, 2453, 1647, 3020,  743,  520,\n",
       "       2373, 4865, 4248, 1942, 4248, 4248, 4045, 2373, 3409, 3925, 3409,\n",
       "        760, 3800, 1823, 3434, 3434, 2378, 3409,  760, 2373, 3409, 3434,\n",
       "       3364, 1765, 1942, 1765,  760, 4248, 1942, 3434, 3020, 1279, 2686,\n",
       "       4342, 3239, 3547,  760, 3434, 4248, 1410, 3419, 4248, 2865, 3941,\n",
       "        743, 4248, 1775, 3364, 2373, 1765, 2826, 2597, 3364, 3409, 2373,\n",
       "       1410, 2373])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
